# CIT Benchmarks Generation and CIT Generators Evaluation

* `Benchmarks_CITCompetition_2022`: benchmarks used during the [first edition of the CT Competition](https://fmselab.github.io/ct-competition/index2022.html)
* `Benchmarks_CITCompetition_2023`: benchmarks used during the [second edition of the CT Competition](https://fmselab.github.io/ct-competition/)
* `Benchmarks_FollowUp_CITCompetition_2022`: benchmarks used during the follow-up of the first edition of the CT Competition
* `BenckmarkGenerator`: Java code for the generation of benchmarks, with different categories depending on the edition of the CT Competition.
* `ToolEvaluator`: given the test suites and the generation times, extract the ranking for each tool


https://fmselab.github.io/ct-competition/
